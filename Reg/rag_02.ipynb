{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f667066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG + Cache + Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e6cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict,Any\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "class RagSystem:\n",
    "    '''\n",
    "        ChromaDB + openai 임베딩\n",
    "    '''\n",
    "    def __init__(self,name='temp'):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.get_or_create_collection(name = name)\n",
    "        self.embed_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key = openai.api_key,\n",
    "            model_name = 'text-embedding-3-small'\n",
    "        )\n",
    "        if len(self.collection.get()) == 0:\n",
    "            docs = [\n",
    "                ('대한민국 수도','서울입니다.'),\n",
    "                ('미국 수도','워싱턴 DC 입니다.'),\n",
    "                ('AI 정의','인간의 지능을 모방한 기술입니다.')\n",
    "            ]\n",
    "            for doc_id ,(title,text) in enumerate(docs):\n",
    "                self.collection.add(\n",
    "                    documents=[text],\n",
    "                    metadata = [{'title':title}],\n",
    "                    ids = [str(doc_id)],\n",
    "                    embedding_function = self.embed_fn\n",
    "                )\n",
    "    def query(self, question:str) -> str:\n",
    "        embedding = self.embed_fn(question)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings = [embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "        if results['documents'][0]:\n",
    "            doc_text = results['documents'][0][0]\n",
    "            return f'RAG 기반 답변 : {doc_text}'\n",
    "        else: # 문서에 없으면\n",
    "            client = OpenAI()\n",
    "            response = client.chat.completions.create(\n",
    "                model='gpt-5-nano',                    \n",
    "                messages=[{\n",
    "                    'role':'user',\n",
    "                    'content' : question\n",
    "                }] \n",
    "            )       \n",
    "            return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7887b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "import os\n",
    "\n",
    "\n",
    "def call_llm(question:str) -> str:\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-5-nano',                    \n",
    "        messages=[{\n",
    "            'role':'user',\n",
    "            'content' : question\n",
    "        }] \n",
    "    )       \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "class SimpleCache:\n",
    "  def __init__(self):\n",
    "    self.cache = {}  # 딕셔너리\n",
    "    self.hits = 0\n",
    "    self.misses = 0\n",
    "  def get(self, key):\n",
    "    if key in self.cache:\n",
    "      self.hits += 1\n",
    "      return self.cache[key]\n",
    "    self.misses += 1\n",
    "    return None\n",
    "  def set(self,key,value):\n",
    "    self.cache[key] = value\n",
    "  def state(self):\n",
    "    total = self.hits + self.misses\n",
    "    hit_rate = self.hits / total*100 if total > 0 else 0\n",
    "    return{\n",
    "        'hits':self.hits,\n",
    "        'misses':self.misses,\n",
    "        'hit_rate':hit_rate\n",
    "    }\n",
    "\n",
    "class SemanticCache:\n",
    "  def __init__(self,name = 'semantic_cache'):\n",
    "    self.client = chromadb.Client()\n",
    "    self.embed_fn = OpenAIEmbeddingFunction(\n",
    "      api_key=os.getenv('OPENAI_API_KEY'),\n",
    "      model_name=\"text-embedding-3-small\"\n",
    "    )\n",
    "    self.collection = self.client.get_or_create_collection(\n",
    "        name = name,\n",
    "        embedding_function=self.embed_fn,\n",
    "        metadata={'hnsw:space':'cosine'}\n",
    "    )\n",
    "  def get(self,query,threshold=0.20):\n",
    "    results = self.collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results = 1\n",
    "    )\n",
    "    # print(f'get results : {results}')\n",
    "\n",
    "    if results['distances'][0] and results['distances'][0][0] < threshold:\n",
    "      return results['metadatas'][0][0]['response']\n",
    "    return None\n",
    "  def set(self, query, response):\n",
    "    import uuid  # unique id 를 자동 생성\n",
    "    self.collection.add(\n",
    "        documents=[query],\n",
    "        metadatas=[{'response':response}],\n",
    "        ids=[str(uuid.uuid4())]\n",
    "    )\n",
    "\n",
    "class MulltiLevelCache:\n",
    "  def __init__(self) -> None:\n",
    "    self.l1_cach = SimpleCache()  # 메모리방식 dictionary   완전일치\n",
    "    self.l2_cach = SemanticCache() # ChoromaDB 벡터DB  유사도방식\n",
    "  def stats(self):\n",
    "    print(f'L1 catch: {self.l1_cach.cache}')\n",
    "  def get(self,key):\n",
    "    cached = self.l1_cach.get(key)\n",
    "    if cached:\n",
    "      print('L1 cache')\n",
    "      return cached\n",
    "    cached = self.l2_cach.get(key)\n",
    "    if cached:\n",
    "      print('L2 cache')\n",
    "      self.l1_cach.set(key,cached)\n",
    "      return cached\n",
    "    # LLM 호출\n",
    "    print('LLM')\n",
    "    response = call_llm(key)\n",
    "    self.l1_cach.set(key,response)\n",
    "    self.l2_cach.set(key,response)\n",
    "    return response\n",
    "class GuardrailsSystem: # 사용자 방식\n",
    "    def __init__(self):\n",
    "        self.bad_words = ['욕설','비속어','나쁜말']\n",
    "    def validate_input(self, text:str):\n",
    "        if len(text.strip()) == 0:\n",
    "            return False, '입력이 비어 있습니다'\n",
    "        for b in self.bad_words:\n",
    "            if b in text:\n",
    "                return False, '입력에 허용되지 않는 단어가 포함되어 있습니다.'\n",
    "        return True, 'ok'\n",
    "    def validate_output(self, text:str):\n",
    "        for b in self.bad_words:\n",
    "            if b in text:\n",
    "                return False, '출력에 허용되지 않는 단어가 포함되어 있습니다.'\n",
    "        return True, 'ok'\n",
    "\n",
    "class CacheSystem:\n",
    "    def __init__(self):\n",
    "        self.cache = MulltiLevelCache()\n",
    "    def get(self, question:str):\n",
    "        return self.cache.get(question)\n",
    "    def set(self, question:str, answer:str):\n",
    "        self.cache.l1_cach.set(question,answer)\n",
    "        self.cache.l2_cach.set(question,answer)\n",
    "\n",
    "class LLMApplication:\n",
    "    def __init__(self):\n",
    "        self.rag = RagSystem()\n",
    "        self.cache = CacheSystem()\n",
    "        self.guardrails = GuardrailsSystem()\n",
    "    def query(self, question:str):\n",
    "        # 1.입력 검증\n",
    "        valid,msg = self.guardrails.validate_input(question)\n",
    "        if not valid:\n",
    "            return \"error\"\n",
    "        # 2.캐쉬 확인\n",
    "        cached  = self.cache.get(question)\n",
    "        if cached:\n",
    "            return cached\n",
    "        # 3.RAG 실행\n",
    "        response = self.rag.query(question)\n",
    "        # 4.출력 검증\n",
    "        valid,msg = self.guardrails.validate_input(response)\n",
    "        if not valid:\n",
    "            return \"error\"\n",
    "        self.cache.set(question,response)\n",
    "        return response   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8780c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM\n",
      "서울특별시(일반적으로 서울)입니다.\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "app = LLMApplication()\n",
    "question = \"대한민국의 수도는 어디인가요?\"\n",
    "answer = app.query(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771527e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
