{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbff81f",
   "metadata": {},
   "source": [
    "í•œêµ­ì–´ ì˜í™” ë¦¬ë·° - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede73279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.10.29</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ëª°ì…í• ìˆ˜ë°–ì— ì—†ë‹¤. ì–´ë µê²Œ ìƒê°í•  í•„ìš”ì—†ë‹¤. ë‚´ê°€ ì „íˆ¬ì— ì°¸ì—¬í•œë“¯ ì†ì— ë•€ì´ë‚¨.</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.26</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì´ì „ ì‘í’ˆì— ë¹„í•´ ë” í™”ë ¤í•˜ê³  ìŠ¤ì¼€ì¼ë„ ì»¤ì¡Œì§€ë§Œ.... ì „êµ­ ë§›ì§‘ì˜ ìŒì‹ë“¤ì„ í•œë° ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.24</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì´ ì •ë„ë©´ ë³¼ë§Œí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŒ!</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.10.22</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì¬ë¯¸ìˆë‹¤</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.10.20</td>\n",
       "      <td>ì¸í”¼ë‹ˆí‹° ì›Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14720</th>\n",
       "      <td>ì–´ë¥¸ë“¤ì„ ìœ„í•œ ë™í™”    ì •ë§ ì˜¤ëœë§Œì—  ì¢‹ì€ ì• ë‹ˆë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤     ê°€ì¡±ì˜ ì†Œì¤‘...</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>ì½”ì½”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14721</th>\n",
       "      <td>ë””ì¦ˆë‹ˆëŠ” ëª»í•´ë„ ë³¸ì „ì€ í•œë‹¤.</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>ì½”ì½”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14722</th>\n",
       "      <td>ê°€ì¡±ì„ ìœ„í•œ ì˜í™”... ê´œì°®ì€ ì˜í™”.~~~</td>\n",
       "      <td>8</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>ì½”ì½”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14723</th>\n",
       "      <td>ê°„ë§Œì— ì œëŒ€ë¡œ ì˜ì§œì—¬ì§„ ê°ë³¸ì˜ ì˜í™”ë¥¼ ë´¤ë„¤ ì—¬ìš´ì´ ì•„ì§ë„ ë‚¨ì•„~ì–´ë¥¸ì„ ìœ„í•œ ì• ë‹ˆ~</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>ì½”ì½”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14724</th>\n",
       "      <td>í•œêµ­ê°œë´‰ì„ ëˆˆë¹ ì§€ê²Œ ê¸°ë‹¤ë¦° ë³´ëŒì´ìˆë‹¤ ê¹¨ìš°ì¹˜ëŠ”ê²Œ ë§ì€ ì˜í™”</td>\n",
       "      <td>10</td>\n",
       "      <td>2018.01.12</td>\n",
       "      <td>ì½”ì½”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14725 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rating        date  \\\n",
       "0                                 ëˆ ë“¤ì¸ê±´ í‹°ê°€ ë‚˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ í•˜í’ˆë§Œ       1  2018.10.29   \n",
       "1           ëª°ì…í• ìˆ˜ë°–ì— ì—†ë‹¤. ì–´ë µê²Œ ìƒê°í•  í•„ìš”ì—†ë‹¤. ë‚´ê°€ ì „íˆ¬ì— ì°¸ì—¬í•œë“¯ ì†ì— ë•€ì´ë‚¨.      10  2018.10.26   \n",
       "2      ì´ì „ ì‘í’ˆì— ë¹„í•´ ë” í™”ë ¤í•˜ê³  ìŠ¤ì¼€ì¼ë„ ì»¤ì¡Œì§€ë§Œ.... ì „êµ­ ë§›ì§‘ì˜ ìŒì‹ë“¤ì„ í•œë° ...       8  2018.10.24   \n",
       "3                                    ì´ ì •ë„ë©´ ë³¼ë§Œí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŒ!       8  2018.10.22   \n",
       "4                                                   ì¬ë¯¸ìˆë‹¤      10  2018.10.20   \n",
       "...                                                  ...     ...         ...   \n",
       "14720  ì–´ë¥¸ë“¤ì„ ìœ„í•œ ë™í™”    ì •ë§ ì˜¤ëœë§Œì—  ì¢‹ì€ ì• ë‹ˆë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤     ê°€ì¡±ì˜ ì†Œì¤‘...      10  2018.01.12   \n",
       "14721                                   ë””ì¦ˆë‹ˆëŠ” ëª»í•´ë„ ë³¸ì „ì€ í•œë‹¤.       7  2018.01.12   \n",
       "14722                            ê°€ì¡±ì„ ìœ„í•œ ì˜í™”... ê´œì°®ì€ ì˜í™”.~~~       8  2018.01.12   \n",
       "14723      ê°„ë§Œì— ì œëŒ€ë¡œ ì˜ì§œì—¬ì§„ ê°ë³¸ì˜ ì˜í™”ë¥¼ ë´¤ë„¤ ì—¬ìš´ì´ ì•„ì§ë„ ë‚¨ì•„~ì–´ë¥¸ì„ ìœ„í•œ ì• ë‹ˆ~      10  2018.01.12   \n",
       "14724                   í•œêµ­ê°œë´‰ì„ ëˆˆë¹ ì§€ê²Œ ê¸°ë‹¤ë¦° ë³´ëŒì´ìˆë‹¤ ê¹¨ìš°ì¹˜ëŠ”ê²Œ ë§ì€ ì˜í™”      10  2018.01.12   \n",
       "\n",
       "        title  \n",
       "0      ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "1      ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "2      ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "3      ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "4      ì¸í”¼ë‹ˆí‹° ì›Œ  \n",
       "...       ...  \n",
       "14720      ì½”ì½”  \n",
       "14721      ì½”ì½”  \n",
       "14722      ì½”ì½”  \n",
       "14723      ì½”ì½”  \n",
       "14724      ì½”ì½”  \n",
       "\n",
       "[14725 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
    "df = pd.read_csv(url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878c83e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„°: 8835, í‰ê°€ ë°ì´í„°: 2945, í…ŒìŠ¤íŠ¸ ë°ì´í„°: 2945\n"
     ]
    }
   ],
   "source": [
    "# rating 6ì´ìƒì´ë©´ ê¸ì •  ë¼ë²¨ ìƒì„±\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# yë¥¼ ìƒì„±í•´ì„œ reviewì»¬ëŸ¼ì´ x ë°ì´í„° ë¶„í•  ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "df['y'] = np.where(df['rating'] >= 6, 1, 0)\n",
    "# ë°ì´í„°ì…‹ì„ í•™ìŠµ í‰ê°€ë¡œ ë‚˜ëˆˆë‹¤ x_train, x_val, x_test\n",
    "x_, x_test, y_, y_test = train_test_split(df['review'], df['y'], test_size=0.2, random_state=42, stratify=df['y'])\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_, y_, test_size=0.25, random_state=42, stratify=y_)\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(x_train)}, í‰ê°€ ë°ì´í„°: {len(x_val)}, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f49c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from datasets import load_metric # 2022 ì´í›„ë¡œ Huggingface datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì´ì „\n",
    "# old system -> legacy system\n",
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")  #  legacy system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66eb60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    '''\n",
    "    Args:\n",
    "        eval_pred: logits, labelsë¥¼ ê°€ì§€ê³  ì‡ëŠ” dataset\n",
    "    Returns:\n",
    "        accuracy\n",
    "    '''\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71a104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ìƒì„± í´ë˜ìŠ¤(ìƒì†)\n",
    "# __init__, __getitem__, __len__ \n",
    "# x, y      ê°ê° í…ì„œ     yì˜ ê°œìˆ˜\n",
    "from torch.utils.data import Dataset\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0411cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•ˆ', '##ë…•', '##í•˜', '##ì„¸', '##ìš”', '.', 'ë°˜', '##ê°‘', '##ìŠµ', '##ë‹ˆë‹¤']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\") # ë‹¤êµ­ì–´ BERT í† í¬ë‚˜ì´ì €\n",
    "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤\"))  # í† í°í™” ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fab5d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer('ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "469104f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2022ë…„ ì´í›„ë¡œëŠ” Auto~~ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ê¶Œì¥(ë²¤ë”ì‚¬ì—ì„œ ì—…ë°ì´íŠ¸ì‹œ ìœ ë¦¬)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "input = tokenizer('ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤')\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dc2ac40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     26\u001b[39m trainer = Trainer(\n\u001b[32m     27\u001b[39m     model=model,                         \u001b[38;5;66;03m# the instantiated ğŸ¤— Transformers model to be trained\u001b[39;00m\n\u001b[32m     28\u001b[39m     args=training_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     compute_metrics=compute_metrics      \u001b[38;5;66;03m# evaluation metrics\u001b[39;00m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Trainerê°ì²´ìƒì„±  ë° í•™ìŠµ(ë¯¸ì„¸ì¡°ì •)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# í‰ê°€\u001b[39;00m\n\u001b[32m     36\u001b[39m trainer.evaluate(eval_dataset=test_dataset)  \u001b[38;5;66;03m# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\accelerate\\data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mOurDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     item = {key: torch.tensor(\u001b[43mval\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m().detach()) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encodings.items()}\n\u001b[32m     11\u001b[39m     item[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m] = torch.tensor(\u001b[38;5;28mself\u001b[39m.labels[idx])\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    "# mBERT + Trainer ë¡œ ë¯¸ì„¸ì¡°ì •(Fine-Tuning) í•˜ê¸°\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# í† í°í™”\n",
    "train_encodings = tokenizer(list(x_train), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(x_val), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(x_test), truncation=True, padding=True, max_length=128)\n",
    "# Datasetìƒì„±\n",
    "train_dataset = OurDataset(train_encodings, list(y_train))\n",
    "val_dataset = OurDataset(val_encodings, list(y_val))\n",
    "test_dataset = OurDataset(test_encodings, list(y_test))\n",
    "# ì‚¬ì „í•™ìŠµëœ ë‹¤êµ­ì–´ BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "# ë¶„ë¥˜ ëª¨ë¸  ìƒì„±\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\"            # evaluation strategy to adopt during training\n",
    ")\n",
    "# Trainerì— ì‚¬ìš©í•  Arguments ì…‹íŒ…\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # evaluation metrics\n",
    ")\n",
    "# Trainerê°ì²´ìƒì„±  ë° í•™ìŠµ(ë¯¸ì„¸ì¡°ì •)\n",
    "trainer.train()\n",
    "# í‰ê°€\n",
    "trainer.evaluate(eval_dataset=test_dataset)  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ í‰ê°€\n",
    "# ì˜ˆì¸¡\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predictions  # ì˜ˆì¸¡ ê²°ê³¼ logits, label_ids, metrics í¬í•¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
