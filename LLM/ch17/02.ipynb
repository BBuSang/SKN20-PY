{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6811c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trantormer는 encoder -> decoder\n",
    "# encoder 를 이용해서 만든 언어모델 BERT : 감성분류, 스팸, 키워드 추출, 유사도 측정 --> 추출\n",
    "# decoder 를 이용해서 만든 언어모델 GPT : 언어 추론 요약, QA 챗본 --> 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b967c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert가 잘하는 것 : 분류, 빈칸 추론, 문장 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "187d07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acde0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=8002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = BertForMaskedLM.from_pretrained('skt/kobert-base-v1')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d89682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\31799\\.cache\\huggingface\\hub\\models--klue--bert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3859b0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.6930,  8.9020, -4.5985,  ..., -5.3556, -3.0629, -2.7361]])\n",
      "우리나라\n",
      "산\n",
      "[UNK]\n",
      "서울\n",
      "한반도\n"
     ]
    }
   ],
   "source": [
    "# 1. 상식 추론\n",
    "import torch\n",
    "text = '옛날옛적에 호랑이가 [MASK]에 살았어요.'\n",
    "inputs = tokenizer(text,return_tensors='pt')\n",
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "predictions = outputs.logits\n",
    "print(predictions[0,mask_token_index,:])\n",
    "masked_prediction = predictions[0,mask_token_index,:].topk(5)\n",
    "for i, index_t in enumerate(masked_prediction.indices[0]):\n",
    "  index = index_t.item()\n",
    "  print(tokenizer.decode([index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9848cfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁안녕', '하', '세', '요.', '▁한국어', '▁G', 'P', 'T', '-2', '▁입', '니다.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2',\n",
    "bos_token = '<s>', eos_token =  '</s>', unk_token = '<unk>',\n",
    "pad_token = '<pad>', mask_token = '<mask>')\n",
    "tokenizer.tokenize('안녕하세요. 한국어 GPT-2 입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c574622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이자, ’파워풀한 팀플레이’는 지난해 5월 KIBA ‘월드 오브 워크래프트’에서 보여준 열정과 역량이 더욱 빛날 것이라고 밝혔다.\n",
      "오대엽 원장은 “지난해에 이어 올해도 두 번째 출전하는 WCG는 월드 시리즈와 달리 국가대표 선발전에 국한되지 않고 모든 연령층의 게이머들과 함께 즐길 수 있는 대회로 자리매김할 것”이라며 “이번 대회를 통해 한국 대표팀의 선전과 더불어 국내 e스포츠 팬들이 한층 더 발전된 플레이를 경험했으면 한다”고 말했다.\n",
      "KeSPA 측은 오는 6월까지 시즌을 진행하며 우승은 물론 준우승과 3위 입상자는 별도로 시상하고 총상금 1,000만원을 수여한다. 경기도문화의전당은 26일 성남시 분당구 야탑동 '경기도문화원'의 개관을 기념, 이달 말까지 입장권 할인행사를 갖는다고 27일 발표했다.\n",
      "경기문화재단은 이번 행사를 위해 경기지역 19개 문화원의 입주자 총 8000여명을 대상으로 공연·체험 등을 무료로 진행하고 있다.\n",
      "주말 현장 신청도 가능하다.\n",
      "성남시립도서관에서는 다음 달 1일부터 9월 28일까지, 서울 도봉도서관은 10월 30일부터 12월 11일까지로 5일간 각각 휴관한다.\n",
      "또 시흥서라벌아트센터에서 열리는 문학콘텐츠 영화제에도 관람객들을 초대하기\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "model.eval()\n",
    "# 프롬프트 엔지니어링\n",
    "prompt = f'{text}\\n\\한줄 요약 : '\n",
    "# 인코딩\n",
    "outputs = tokenizer(prompt, return_tensors='pt')\n",
    "# 모델 추론(생성)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        max_length=256,\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        bos_token_id = tokenizer.bos_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
