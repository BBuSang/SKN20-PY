{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9gQ3gP62ZcW"
      },
      "source": [
        "KoBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "G2dIQFDy5LRS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4z73QTC3s3-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "ERROR: Invalid requirement: \"'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\": Expected package name at the start of dependency specifier\n",
            "    'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\n",
            "    ^\n",
            "Hint: = is not a valid operator. Did you mean == ?\n",
            "'subdirectory'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "%pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MrRItwWK4vyb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\31799\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\31799\\.cache\\huggingface\\hub\\models--skt--kobert-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "tokenizer.encode(\"한국어 모델을 공유합니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fHcNdKPf6YoM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer('한국어 모델을 공유합니다.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tVRLYpSS2ba_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "text = \"한국어 모델을 공유합니다.\"\n",
        "inputs = tokenizer.batch_encode_plus([text])\n",
        "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
        "              attention_mask = torch.tensor(inputs['attention_mask']))\n",
        "out.pooler_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WuuCKmL66_6B"
      },
      "outputs": [],
      "source": [
        "# 데이터셋클래스\n",
        "import torch\n",
        "class OurDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eGt5Er_I-VHZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14725\n",
            "1472\n"
          ]
        }
      ],
      "source": [
        "# 데이터 로드 및 분할(여기서는 성능상 일부 데이터만 사용)\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# 데이터셋 로드\n",
        "url = \"https://drive.google.com/uc?id=1KOKgZ4qCg49bgj1QNTwk1Vd29soeB27o\"\n",
        "df = pd.read_csv(url)\n",
        "print(len(df))\n",
        "df = df.sample(frac=0.1)\n",
        "print(len(df))\n",
        "X = df.review.tolist()\n",
        "y = (df.rating >= 6).values.astype(int)\n",
        "x_,x_test,y_,y_test = train_test_split(X,y,stratify=y,random_state=42,test_size=0.2)\n",
        "x_train,x_val,y_train,y_val = train_test_split(x_,y_,stratify=y_, random_state=42,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OxrAllZS8rM_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\31799\\AppData\\Local\\Temp\\ipykernel_17384\\1213381581.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6635477997488894\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# 토큰화\n",
        "train_input = tokenizer(x_train, truncation=True, padding=True,return_tensors='pt')\n",
        "val_input = tokenizer(x_val, truncation=True, padding=True, max_length=512,return_tensors='pt')\n",
        "test_input = tokenizer(x_test, truncation=True, padding=True, max_length=512,return_tensors='pt')\n",
        "# DataSet 생성\n",
        "train_dataset = OurDataset(train_input, y_train)\n",
        "val_dataset = OurDataset(val_input, y_val)\n",
        "test_dataset = OurDataset(test_input, y_test)\n",
        "# 데이터로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16,)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16,)\n",
        "# KoBERT 한국어 전용 모델 로드\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "import torch.nn as nn\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, pretrained_model, token_size, num_labels):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.token_size = token_size\n",
        "        self.num_labels = num_labels\n",
        "        # 분류기 정의\n",
        "        self.clf = nn.Linear(self.token_size, self.num_labels)\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.pretrained_model(**inputs) # [batch_size, embeddding_dim, num_labels]\n",
        "        bert_clf_token = outputs.last_hidden_state[:,0,:] # [CLS] 토큰\n",
        "\n",
        "        return self.clf(bert_clf_token)\n",
        "model = MyModel(model, token_size=model.config.hidden_size, num_labels=2)\n",
        "# BERT를 포함한 신경망 모델\n",
        "# 학습 - 미니배치\n",
        "  # device\n",
        "import torch\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "  # optimize\n",
        "from torch import optim\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "  # 학습 스케줄러\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "num_training_steps = len(train_loader)  # 1 epoch에 대한 학습 스텝 수\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
        "                                            num_warmup_steps=200, \n",
        "                                            num_training_steps=num_training_steps)\n",
        "  # epoch수만큼 loop\n",
        "total_loss = 0\n",
        "for batch in train_loader:\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  inputs = {k : v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "  labels = batch['labels'].to(device)\n",
        "  outputs = model(inputs)\n",
        "  loss = criterion(outputs, labels.long())\n",
        "  loss = criterion(outputs, labels)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  scheduler.step()\n",
        "  total_loss += loss.item()\n",
        "print(total_loss / len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\31799\\AppData\\Local\\Temp\\ipykernel_17384\\1213381581.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n"
          ]
        }
      ],
      "source": [
        "# val 데이터로 해당 epoch  학습된 모델을 평가\n",
        "import evaluate\n",
        "with torch.no_grad():\n",
        "    metric = evaluate.load('accuracy')\n",
        "    model.eval()\n",
        "    for batch in val_loader:\n",
        "        inputs = {k : v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions = torch.argmax(outputs, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=labels)\n",
        "    metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['조금 어설픈듯한 부분도 있었지만 그런 부분을 잊을만큼 충분히 감동적인 영화였다',\n",
              " '올해 제가 본 것중 제일 감동적이에요',\n",
              " '여배우가 아쉽네요',\n",
              " '웅장한 스케일  어마무시한  세계관  의미심장한 결말 .속편을 위한 떡밥을 찾기도 잊은체  영화에 빠져들게 만드는 스토리 개연성 있는 짜임새 전개에 죄강 빌런 타노스의 매력 까지  눈코뜰새 없이 몰입해서 봣네요. 잘차려진 부페인데 어느하나 맛 없는게 없는 영화. 캐릭간의 스토리 편중 공평하고 이만하면  짪은 시간에 그 거대한 세계관을 잘담아 보여줬다 봅니다.  보는내내 심장이 바운스 바운스 ㅋㅋ',\n",
              " '순수 평화시위인것으로 위장한 왜곡된 영화~~! 무기고 탈취 총기무장 한 반군 내전 총격전 이였음~~! .']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2 = df.reset_index(drop=True)\n",
        "df2[-5:].review.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(df2[-5:].review.tolist(), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "outputs = model(inputs)\n",
        "preds = torch.softmax(outputs, dim=-1)\n",
        "preds = torch.argmax(preds, dim=-1)\n",
        "preds"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
