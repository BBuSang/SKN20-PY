{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROOrxwYy6My5",
        "outputId": "85539560-3533-437b-c12f-4518f76994f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.5)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.3.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: httpx<1.0.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\31799\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tMBRltXI5zLP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\31799\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 필수 라이브러리 임포트\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Transformers 라이브러리\n",
        "from transformers import (\n",
        "    pipeline,                              # 고수준 API - 가장 쉬운 방법\n",
        "    AutoTokenizer,                         # 자동 토크나이저\n",
        "    AutoModelForQuestionAnswering,         # QA 모델 자동 로더\n",
        "    DistilBertTokenizerFast,              # DistilBERT 고속 토크나이저\n",
        "    DistilBertForQuestionAnswering,        # DistilBERT QA 모델\n",
        "    ElectraTokenizer,                      # ELECTRA 토크나이저 (한글)\n",
        "    ElectraForQuestionAnswering,           # ELECTRA QA 모델 (한글)\n",
        "    DefaultDataCollator,                   # 기본 데이터 콜레이터\n",
        "    TrainingArguments,                     # 학습 하이퍼파라미터\n",
        "    Trainer,                               # 범용 트레이너\n",
        ")\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsisx17i53k9",
        "outputId": "6aae3d17-e0b7-4766-cb35-0ee943013995"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "answer1 : the process of deriving high-quality information from text\n",
            "answer2 : 답변 없음\n",
            "answer1 : the process of deriving high - quality information from text\n"
          ]
        }
      ],
      "source": [
        "question_answer = pipeline(\"question-answering\",model = 'distilbert-base-cased-distilled-squad')\n",
        "\n",
        "context = \"\"\"Text mining, also referred to as text data mining (abbr.: TDM), similar to text analytics,\n",
        "is the process of deriving high-quality information from text. It involves\n",
        "\"the discovery by computer of new, previously unknown information,\n",
        "by automatically extracting information from different written resources.\"\n",
        "Written resources may include websites, books, emails, reviews, and articles.\n",
        "High-quality information is typically obtained by devising patterns and trends\n",
        "by means such as statistical pattern learning. According to Hotho et al. (2005)\n",
        "we can distinguish between three different perspectives of text mining:\n",
        "information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process.\"\"\"\n",
        "\n",
        "question1 = \"What is text mining?\"\n",
        "question2 = \"What are the perspectives of text mining?\"\n",
        "\n",
        "# 질의 응답 수행\n",
        "answer1 = question_answer(context=context, question=question1)\n",
        "answer2 = question_answer(context=context, question=question2)\n",
        "if answer1['score'] < 0.1:\n",
        "  print(f'answer1 : 답변 없음')\n",
        "else:\n",
        "  print(f\"answer1 : {answer1['answer']}\")\n",
        "if answer2['score'] < 0.1:\n",
        "  print(f'answer2 : 답변 없음')\n",
        "else:\n",
        "  print(f\"answer2 : {answer2['answer']}\")\n",
        "\n",
        "# AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(question1, context, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "start_score = outputs.start_logits\n",
        "end_score  = outputs.end_logits\n",
        "answer_start = torch.argmax(start_score)\n",
        "answer_end = torch.argmax(end_score)\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n",
        "print(f\"answer1 : {answer}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR_U591M8FBN",
        "outputId": "22682303-d518-499e-d767-f468d93c93a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 467997.44 examples/s]\n",
            "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 703342.59 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 4000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SQuAD 데이터셋 로드 분석\n",
        "# 스탠포드 대학에서 공개한 질의응답 벤치마크 - Extractive QA 표준\n",
        "squad = load_dataset('squad', split='train[:5000]')\n",
        "squad = squad.train_test_split(test_size=0.2,seed=42)\n",
        "squad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRYGj-PGsg0",
        "outputId": "04afee2f-6a0a-4d66-f294-3f17fb5e8d4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 4000\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXYt-8x_HDuD",
        "outputId": "7629db80-326a-4af7-cc76-151d18ea99fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With the d\n",
            "What estab\n",
            "['Neo-Confucian establishment']\n",
            "[98]\n",
            "Neo-Confucian establishment\n"
          ]
        }
      ],
      "source": [
        "print(squad['train'][0]['context'][:10])\n",
        "print(squad['train'][0]['question'][:10])\n",
        "print(squad['train'][0]['answers']['text'])\n",
        "print(squad['train'][0]['answers']['answer_start'])\n",
        "print(squad['train'][0]['context'][98:98 + len('Neo-Confucian establishment')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6knz_YHpH_Dk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Fine turnnig\n",
        "# 사전학습만 된모델(QA 헤드는 초기화)  distilbert-base-uncased\n",
        "# 한국어 학습이 가능하지만 성능 보장 못하고 비효율적\n",
        "# distilbert-base-uncased 영어전용 한국어를 전처리할때 어간 및 품사등이 달라서 심하게 왜곡\n",
        "# 한국어면 한국어전용 base에 모델에 파인튜닝을 또는 다국어모델에\n",
        "# mBERT  bert-base-multilingual-cased\n",
        "# klue/bert-base 등등\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QA헤드는 아직 학습되지 않음(랜덤 가중치가 적용)\n",
        "test_context = \"\"\"The city is the birthplace of many cultural movements, including the Harlem \n",
        "Renaissance in literature and visual art; abstract expressionism \n",
        "(also known as the New York School) in painting; and hip hop, punk, salsa, disco, \n",
        "freestyle, Tin Pan Alley, and Jazz in music. New York City has been considered \n",
        "the dance capital of the world. The city is also widely celebrated in popular lore, \n",
        "frequently the setting for books, movies, and television programs.\"\"\"\n",
        "    \n",
        "test_question = \"The dance capital of the world is what city in the US?\"\n",
        "\n",
        "# Fine-tuning이 되지않은 모델\n",
        "inputs = tokenizer(test_question, test_context, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "start = torch.argmax(outputs.start_logits)\n",
        "end = torch.argmax(outputs.end_logits)\n",
        "input_ids = inputs['input_ids'][0]\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end+1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning Hugging Face Trainer API를 이용해서 미세조정학습(추가학습)\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"./qa_model\",                # 모델 저장 경로\n",
        "        eval_strategy=\"epoch\",                  # 매 에포크마다 평가\n",
        "        learning_rate=2e-5,                     # 학습률\n",
        "        per_device_train_batch_size=16,         # 학습 배치 크기\n",
        "        per_device_eval_batch_size=16,          # 평가 배치 크기\n",
        "        num_train_epochs=3,                     # 에포크 수\n",
        "        weight_decay=0.01,                      # 가중치 감쇠\n",
        "        logging_steps=100,                      # 100스텝마다 로그\n",
        "        save_strategy=\"epoch\",                  # 에포크마다 저장\n",
        "        save_total_limit=2,                     # 최근 2개만 유지\n",
        "        load_best_model_at_end=True,            # 최고 모델 로드\n",
        "        metric_for_best_model=\"eval_loss\",      # 평가 손실 기준\n",
        "        fp16=True,                              # Mixed Precision (GPU만)\n",
        "        push_to_hub=False,                      # Hub 업로드 안 함\n",
        "        report_to=\"none\",                       # 외부 로깅 비활성화\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=squad['train'],\n",
        "    eval_dataset=squad['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DefaultDataCollator(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
